{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c98142b",
   "metadata": {},
   "source": [
    "# <center> Google - Generativeai üåêüíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec64dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# import google.generativeai as genai\n",
    "\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "# genai.configure(api_key=api_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f7ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# try:\n",
    "#    response = model.generate_content(input(\"Digite sua pergunta: \"))\n",
    "#    print(response.text)\n",
    "# except Exception as e:\n",
    "#     print(\"Erro ao consumir a API:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7445c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.generativeai as genai\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "# model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# # Cria uma sess√£o de chat\n",
    "# chat = model.start_chat()\n",
    "# # Defina o prompt de sistema\n",
    "# system_prompt = \"Voc√™ √© um assistente t√©cnico em IA que responde de forma objetiva, clara e sem rodeios. Sempre utilize exemplos em Python.\"\n",
    "# try:\n",
    "#    # Envia o prompt de sistema como a primeira mensagem para definir o comportamento\n",
    "#    chat.send_message(system_prompt)\n",
    "\n",
    "#     # Agora o modelo pode receber a entrada do usu√°rio\n",
    "#    user_prompt = input(\"Digite sua pergunta: \")\n",
    "\n",
    "#    response = chat.send_message(user_prompt)\n",
    "#    print(response.text)\n",
    "# except Exception as e:\n",
    "#    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c1eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain.schema.messages import SystemMessage, HumanMessage\n",
    "# from dotenv import load_dotenv\n",
    "# import os \n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     temperature=0.7,\n",
    "#     google_api_key=api_key\n",
    "# ) \n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(\n",
    "#         content=\"Voc√™ √© um tutor especializado em LangChain. Responda sempre com clareza, de forma objetiva, com exemplos simples em Python. Evite jarg√µes t√©cnicos desnecess√°rios.\"\n",
    "#     ),\n",
    "#     HumanMessage(\n",
    "#         content=\"O que √© LangChain e como ele se relaciona com o GEMINI?\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# try:\n",
    "#     response = llm(messages)\n",
    "#     print(response.content)\n",
    "# except Exception as e:\n",
    "#     print(f\"Ocorreu um erro: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eeb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\livianavarro-ieg\\AppData\\Local\\Temp\\ipykernel_15772\\1871694576.py:59: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "C:\\Users\\livianavarro-ieg\\AppData\\Local\\Temp\\ipykernel_15772\\1871694576.py:68: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "C:\\Users\\livianavarro-ieg\\AppData\\Local\\Temp\\ipykernel_15772\\1871694576.py:79: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resposta_agente = agent.run(pergunta_usuario)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mPara responder a essa pergunta, preciso analisar o conte√∫do dos arquivos de texto dispon√≠veis e identificar o cantor mais proeminente na categoria de m√∫sica pop.\n",
      "\n",
      "Action: ler_arquivo\n",
      "Action Input: musica_pop.txt\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAtualmente aborda temas rom√¢nticos, especialmente sobre o amor entre jovens\n",
      "Tornou-se um g√™nero musical popular em todo o mundo\n",
      "Am√©rica Latina possui artistas relevantes como Shakira, Jennifer Lopez e Ricky Martin\n",
      "Michael Jackson √© conhecido como \"O Rei do Pop\", considerado o maior cantor do g√™nero\n",
      "Spice Girls se destacaram com hits como \"If You Wanna Be My Lover\" e \"Wannabe\"\n",
      "Justin Bieber fez sucesso global durante a adolesc√™ncia a partir dos anos 2010\n",
      "Bruno Mars ganhou destaque com uma proposta inovadora, sendo chamado de \"Pr√≠ncipe do Pop\"\n",
      "Bruno Mars √© considerado por muitos como o sucessor de Michael Jackson\n",
      "Taylor Swift se tornou uma das maiores cantoras do pop com reconhecimento mundial\n",
      "O pop √© atualmente o g√™nero musical mais popular do mundo\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mO arquivo \"musica_pop.txt\" menciona Michael Jackson como \"O Rei do Pop\" e o considera o maior cantor do g√™nero.\n",
      "\n",
      "Final Answer: Michael Jackson\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Resposta do Agente (baseada nos arquivos existentes): Michael Jackson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\livianavarro-ieg\\AppData\\Local\\Temp\\ipykernel_15772\\1871694576.py:96: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output_juiz = llm_juiz(prompt_formatado)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avalia√ß√£o do Juiz (Gemini Pro):\n",
      "Como juiz de IA, avalio a afirma√ß√£o:\n",
      "\n",
      "**SIM**\n",
      "\n",
      "**Justificativa:**\n",
      "\n",
      "A \"afirma√ß√£o\" apresentada √© simplesmente um nome pr√≥prio: \"Michael Jackson\". Um nome pr√≥prio, por si s√≥, n√£o √© uma proposi√ß√£o que possa ser verdadeira ou falsa no sentido l√≥gico de uma frase completa ou uma afirma√ß√£o sobre algo.\n",
      "\n",
      "No entanto, se interpretarmos \"correta\" no sentido de se o nome se refere a uma entidade real e reconhecida, ent√£o a resposta √© SIM. Michael Jackson foi uma pessoa real, um artista mundialmente famoso. Portanto, o nome \"Michael Jackson\" √© uma refer√™ncia correta a essa figura hist√≥rica e cultural.\n",
      "\n",
      "N√£o h√° erro factual ou conceitual na apresenta√ß√£o deste nome como uma refer√™ncia a essa pessoa.\n",
      "\n",
      " O Juiz considerou a informa√ß√£o factual.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import ChatPromptTemplate  # <-- IMPORT NECESS√ÅRIO\n",
    "\n",
    "# Carrega as vari√°veis de ambiente\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Inicializa o modelo LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\n",
    "\n",
    "# Pasta com os arquivos\n",
    "pasta_arquivos = \"./arquivos/\"\n",
    "\n",
    "# Lista os arquivos da pasta\n",
    "try:\n",
    "    if os.path.isdir(pasta_arquivos):\n",
    "        arquivos_disponiveis = [\n",
    "            os.path.join(pasta_arquivos, nome)\n",
    "            for nome in os.listdir(pasta_arquivos)\n",
    "            if os.path.isfile(os.path.join(pasta_arquivos, nome))\n",
    "        ]\n",
    "    else:\n",
    "        print(f\"Pasta '{pasta_arquivos}' n√£o encontrada.\")\n",
    "        arquivos_disponiveis = []\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao listar arquivos: {e}\")\n",
    "    arquivos_disponiveis = []\n",
    "\n",
    "# Nomes dos arquivos\n",
    "nomes_exibicao = [os.path.basename(arquivo) for arquivo in arquivos_disponiveis]\n",
    "nomes_validos = set(nomes_exibicao)\n",
    "\n",
    "# Fun√ß√£o segura para leitura\n",
    "def ler_arquivo(nome_arquivo: str) -> str:\n",
    "    if nome_arquivo not in nomes_validos:\n",
    "        return f\"Erro: O arquivo '{nome_arquivo}' n√£o est√° na lista de arquivos dispon√≠veis.\"\n",
    "    caminho_completo = os.path.join(pasta_arquivos, nome_arquivo)\n",
    "    try:\n",
    "        with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao ler o arquivo '{nome_arquivo}': {e}\"\n",
    "\n",
    "# Ferramenta\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"ler_arquivo\",\n",
    "        func=ler_arquivo,\n",
    "        description=f\"L√™ o conte√∫do de um arquivo de texto listado em: {', '.join(nomes_exibicao)}\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Mem√≥ria e prompt\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "mensagem_inicial = f\"\"\"Voc√™ √© um especialista em g√™neros musicais.\n",
    "Use a ferramenta 'ler_arquivo' para obter informa√ß√µes sobre estilos musicais, origens e artistas.\n",
    "NUNCA invente nomes de arquivos. Use somente os arquivos existentes:\n",
    "\n",
    "Arquivos dispon√≠veis: {', '.join(nomes_exibicao)}\"\"\"\n",
    "memory.chat_memory.add_user_message(mensagem_inicial)\n",
    "\n",
    "# Inicializa agente\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Pergunta\n",
    "pergunta_usuario = \"Qual o maior cantor de musica pop?\"\n",
    "try:\n",
    "    resposta_agente = agent.run(pergunta_usuario)\n",
    "    print(f\"\\nResposta do Agente (baseada nos arquivos existentes): {resposta_agente}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao executar o agente: {e}\")\n",
    "\n",
    "# Juiz\n",
    "try:\n",
    "    llm_juiz = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-04-17\", google_api_key=api_key)\n",
    "\n",
    "    prompt_juiz_template = \"\"\"Voc√™ √© um juiz de IA. Avalie se a seguinte afirma√ß√£o √© correta\n",
    "(SIM ou NAO) e justifique: \"{afirmacao}\".\"\"\"\n",
    "    prompt_juiz = ChatPromptTemplate.from_template(prompt_juiz_template)\n",
    "\n",
    "    # Formata o prompt com a resposta do agente\n",
    "    prompt_formatado = prompt_juiz.format_messages(afirmacao=resposta_agente)\n",
    "\n",
    "    # Chama o modelo com o prompt j√° formatado\n",
    "    output_juiz = llm_juiz(prompt_formatado)\n",
    "    avaliacao_juiz = output_juiz.content\n",
    "\n",
    "    print(f\"\\nAvalia√ß√£o do Juiz (Gemini Pro):\\n{avaliacao_juiz}\")\n",
    "    if \"NAO\" in avaliacao_juiz.upper():\n",
    "        print(\"\\n O Juiz identificou uma poss√≠vel alucina√ß√£o!\")\n",
    "    else:\n",
    "        print(\"\\n O Juiz considerou a informa√ß√£o factual.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao executar o juiz: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ca8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
